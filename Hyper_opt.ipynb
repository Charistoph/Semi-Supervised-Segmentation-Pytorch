{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize, forest_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.utils import use_named_args\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from data_functions import build_loader\n",
    "from model_functions import build_segmentation_model\n",
    "from training_functions import train\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from training_functions import *\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=0.14.0 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-optimize) (1.1.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-optimize) (1.14.5)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-optimize) (0.19.1)\n",
      "Installing collected packages: scikit-optimize\n",
      "Successfully installed scikit-optimize-0.5.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize\n",
    "!pip install imgaug\n",
    "!pip install opencv-python\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_string(hyperdict,logdir=\"/\"):\n",
    "    s=logdir\n",
    "    for key,values in hyperdict.items():\n",
    "        s=s+\"_\"+key+\"_{}_\".format(np.round(values, decimals=12))\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_epoch=50\n",
    "max_epoch=500\n",
    "batch_size=12\n",
    "\n",
    "writer_name_list_eval=['valid/non_ema_loss_eval','valid/ema_loss_eval','valid/total_loss_eval','valid/lovasz_loss_eval','valid/focal_loss_eval','valid/lovasz_loss_ema_eval',\n",
    "                       'valid/focal_loss_ema_eval','valid/unsupervised_loss_eval','valid/iou_score_eval']\n",
    "writer_name_list_train=['train/non_ema_loss','train/ema_loss','train/total_loss','train/lovasz_loss','train/focal_loss','train/lovasz_loss_ema',\n",
    "                       'train/focal_loss_ema','train/unsupervised_loss','train/iou_score']\n",
    "\n",
    "#writer = SummaryWriter(log_dir=\"/home/ubuntu/Kaggle_Pytorch_TGS/plogs2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "\n",
    "# The list of hyper-parameters we want to optimize. For each one we define the bounds,\n",
    "# the corresponding scikit-learn parameter name, as well as how to sample values\n",
    "# from that dimension (`'log-uniform'` for the learning rate)\n",
    "space  = [Real(0.1, 0.5, \"uniform\", name='focal_scaling'),\n",
    "          Integer(int(1), int(batch_size/2), name='second_batch_size'),\n",
    "          Real(0.00000001, 0.0001, \"log-uniform\", name='decay'),\n",
    "          Real(0.01, 0.3, \"uniform\", name='unsupervised_scaling'),\n",
    "          Real(0.1, 0.5, \"uniform\", name='ema_scaling'),\n",
    "          Real(0.1, 0.5, \"uniform\", name='non_ema_scaling'),\n",
    "          Real(0.01, 0.5, \"log-uniform\", name='droppout'),\n",
    "          Real(0.1, 0.5, \"uniform\", name='lovasz_scaling')]\n",
    "\n",
    "# this decorator allows your objective function to receive a the parameters as\n",
    "# keyword arguments. This is particularly convenient when you want to set scikit-learn\n",
    "# estimator parameters\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    logdir = dict_to_string(params)\n",
    "    logdir=\"/home/ubuntu/Kaggle_Pytorch_TGS/plogs2\"+logdir\n",
    "    print(logdir)\n",
    "    writer = SummaryWriter(log_dir=logdir)\n",
    "    \n",
    "    augs = iaa.Sequential([\n",
    "    #iaa.Scale((512, 512)),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Affine(rotate=(-25, 25),mode=\"reflect\",\n",
    "               translate_percent={\"x\": (-0.01, 0.01), \"y\": (-0.01, 0.01)}),\n",
    "    #iaa.Add((-40, 40), per_channel=0.5, name=\"color-jitter\")  \n",
    "    ])\n",
    "    \n",
    "    \n",
    "    train_loader,valid_loader=build_loader(input_img_folder='data/train/images/'\n",
    "                 ,label_folder='data/train/masks/'\n",
    "                 ,test_img_folder='data/test/images/'\n",
    "                 ,second_batch_size=params[\"second_batch_size\"]\n",
    "                 ,batch_size=batch_size\n",
    "                 ,transform=augs\n",
    "                 ,show_image=False\n",
    "                 ,num_workers=4)\n",
    "    \n",
    "    \n",
    "    #in the final training funciotn we will put them too. \n",
    "    segmentation_module,segmentation_ema=build_segmentation_model(\n",
    "    in_arch=\"resnet50_dilated8\",out_arch=\"upernet\" ,droppout=params[\"droppout\"])\n",
    "    optimizer = torch.optim.SGD(\n",
    "        group_weight(segmentation_module),\n",
    "        lr=0.01,\n",
    "        momentum=0.9,\n",
    "        weight_decay=params[\"decay\"])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "    \n",
    "    best_metric=0\n",
    "    wait=0\n",
    "    n_iter=0\n",
    "\n",
    "    \n",
    "    for j in range(max_epoch):\n",
    "        #Trains for one epoch\n",
    "        train(train_loader,segmentation_module,segmentation_ema,optimizer\n",
    "              ,writer=writer\n",
    "              ,lovasz_scaling=params[\"lovasz_scaling\"]\n",
    "              ,focal_scaling=params[\"focal_scaling\"]\n",
    "              ,unsupervised_scaling=params[\"unsupervised_scaling\"]\n",
    "              ,ema_scaling=params[\"ema_scaling\"]\n",
    "              ,non_ema_scaling=params[\"non_ema_scaling\"]\n",
    "              ,train=True\n",
    "              #,test=True\n",
    "              ,writer_name_list=writer_name_list_train\n",
    "              ,second_batch_size=params[\"second_batch_size\"])\n",
    "        \n",
    "        # Does the Evaluation.\n",
    "        metric=train(valid_loader,segmentation_module,segmentation_ema,optimizer\n",
    "              ,writer=writer\n",
    "              ,lovasz_scaling=params[\"lovasz_scaling\"]\n",
    "              ,focal_scaling=params[\"focal_scaling\"]\n",
    "              ,unsupervised_scaling=params[\"unsupervised_scaling\"]\n",
    "              ,ema_scaling=params[\"ema_scaling\"]\n",
    "              ,non_ema_scaling=params[\"non_ema_scaling\"]\n",
    "              ,train=False\n",
    "              #,test=True\n",
    "              ,writer_name_list=writer_name_list_eval\n",
    "              ,second_batch_size=params[\"second_batch_size\"])\n",
    "        scheduler.step(metric)\n",
    "        \n",
    "        #Save best metric and do simple early stopping\n",
    "        if metric > best_metric:\n",
    "            best_metric=metric\n",
    "            wait=0\n",
    "        else:\n",
    "            wait =wait+1\n",
    "        if wait > wait_epoch:\n",
    "            break\n",
    "    print(best_metric)\n",
    "    return -best_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Kaggle_Pytorch_TGS/plogs2/_focal_scaling_0.385182128209__second_batch_size_3__decay_5.801488e-06__unsupervised_scaling_0.218553589945__ema_scaling_0.296447573373__non_ema_scaling_0.412011104765__droppout_0.049905473819__lovasz_scaling_0.331877718809_\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Kaggle_Pytorch_TGS/losses.py:163: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logpt = F.log_softmax(input)\n"
     ]
    }
   ],
   "source": [
    "res_gp = gp_minimize(objective, space, n_calls=50, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
